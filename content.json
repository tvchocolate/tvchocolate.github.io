{"posts":[{"title":"","text":"自编码器笔记 自编码器（autoencoder）是神经网络的一种，其主要目的是经训练后能尝试将输入复制到输出。该网络可以看作两部分组成：能够将输入压缩成潜在空间表征的编码器，由函数h=f(x)h=f(x)h=f(x)表示；能够生成潜在表征的重构的解码器，由函数 r=g(h)r=g(h)r=g(h) 表示。 如果一个自编码器只能简单地学会将处处设置为 g(f(x))=xg(f(x))=xg(f(x))=x ,那么这个自编码器就没什么特别的用处。所以通常向自编码器强加一些约束，使其只能近似地复制，并只能复制与训练数据相似的输入。这样的约束强制模型考虑输入数据的哪些部分需要被优先复制，因此它往往能学习到数据的有用特性。 欠完备自编码器 欠完备（undercomplete）自编码器限制 hhh 的维度比 xxx 小，其强制自编码器捕捉训练数据中最显著的特征。学习过程为最小化loss： L(x,g(f(x)))L(x,g(f(x))) L(x,g(f(x))) 其中 LLL 是一个loss，惩罚 g(f(x))g(f(x))g(f(x)) 与 xxx 的差异。 欠完备自编码器的目的在于通过训练自编码器对输入进行复制从而使得 hhh 捕捉到有关数据分布的有用信息。然而，如果编码器和解码器被赋予过大的容量，自编码器会单纯地执行复制任务而无法学习到数据集的任何有用信息。 正则自编码器 如果在隐藏编码的维数允许与输入相等或者大于输入的过完备（overcomplete）情况下，自编码器学不到任何有关数据分布的有用信息。正则自编码器通过改变损失函数，鼓励任意架构的自编码器模型学习除了将输入复制到输出的其他特性，而不必限制模型容量。 除了这里描述的方法，几乎任何带有潜变量并配有一个推断过程（计算给定输入的潜在表示）的生成模型，都可以看作自编码器的一种特殊形式。 稀疏自编码器 稀疏自编码器简单地在训练时结合编码层的稀疏惩罚 Ω(h)\\Omega(h)Ω(h) 和重构误差： L(x,g(f(x)))+Ω(h)L(x,g(f(x)))+\\Omega(h) L(x,g(f(x)))+Ω(h) 其中 g(h)g(h)g(h) 是解码器的输出，通常 hhh 是编码器的输出，即 h=f(x)h=f(x)h=f(x)。 稀疏自编码器一般用来学习特征，其必须反映训练数据集的独特统计特征，而不是简单地充当恒等函数。Ω(h)\\Omega(h)Ω(h) 与其他的正则项如权重衰减（L2正则）不同，在MAP的近似贝叶斯推断 logp(θ∣x)=logp(x∣θ)+log(θ)logp(\\theta|x)=logp(x|\\theta)+log(\\theta)logp(θ∣x)=logp(x∣θ)+log(θ) 中，正则化的惩罚即 log(θ)log(\\theta)log(θ) 对应于模型参数的先验概率分布。然而正则自编码器由于正则项取决于数据不适用于先验概率的解释，从定义上来说，稀疏惩罚不是一个先验。这也给出了为什么自编码器学到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。 去噪自编码器 1","link":"/2022/08/25/2022-8-25/"},{"title":"","text":"yt-dlp下载b站视频并用ffmpeg批次提取音频 最近在用iTunes导入歌曲，之前是从b站下载视频转码到mp4，提取音频转码到mp3再导入iTunes，这一串流程导致最终听到的歌曲糊的不行，必须改一改这个笨方法。 1. yt-dlp下载视频 之前从b站下载视频一直使用的是youtube-dl工具，最近经常性出现连接不上的问题下载卡住，如下图： 于是尝试使用yt-dlp爬取视频，yt-dlp是一个youtube-dlp的fork，并且基于如今不在活跃的youtube-dlc而开发，因此其使用方式也与youtube-dl高度重合。 下载安装 下载地址：https://github.com/yt-dlp/yt-dlp ，下载安装后记得配置环境变量哦~ 直接使用pip下载安装： python3 -m pip install -U yt-dlp 这里我在conda虚拟环境中用pip直接安装就不需要配置环境变量了。 使用：下载视频 只需要复制网址，并在terminal内使用yt-dlp即可下载视频到当前目录： 1yt-dlp https://www.xxxxxxx.com/xxxx/xxxxx 一些常用参数： 123-a, --batch-file FILE 批次处理文件中的url-o, --output [TYPES:]TEMPLATE 指定输出文件名模板及输出目录-f, --format FORMAT 指定视频格式 关于yt-dlp的更详细文档：https://github.com/yt-dlp/yt-dlp#readme 我在下载时实际使用的命令： 1yt-dlp -a url.txt -o f:/song/video/%(title)s.%(ext)s 不知道为什么b站的视频下载好后都是.m4s后缀的文件。 使用：传入cookies 一些情况下，我们下载一些视频需要进行鉴权（比如：某视频平台需要会员，才可以下载高清视频）。但是，我们浏览器上已经登录会员了，yt-dlp如何调取呢？ 这个时候，就可以用yt-dlp的特性：调取浏览器Cookies： --cookies-from-browser：从浏览器调取cookies，你也可以指定浏览器，比如：--cookies-from-browser chrome就是调取chrome浏览器的cookies。 目前支持的浏览器选项：brave, chrome,chromium, edge, firefox, opera, safari,vivaldi。 示例： 1yt-dlp --cookies-from-browser chrome https://www.xxxxxxx.com/xxxx/xxxxx 2. ffmpeg批次提取音频 下载好视频后，就要从中提取音频，这里使用ffmepg进行成批提取，一行命令搞定： 1for /r %a in (*.m4s) do ffmpeg -i &quot;%~na.m4s&quot; -vn -codec copy &quot;%~na.m4a&quot; 上述命令的含义：在当前目录中遍历.m4s后缀的文件，输入ffmpeg中，复制输入文件的原编解码器，以原文件名加.m4a后缀输出。 .m4a是一种音频文件格式，是以mp4格式所创建的纯音频文件，其只是一种容器，m4a文件能在压缩的同时拥有无损的音质，该格式广泛用于苹果设备中。iTunes支持导入mp3和m4a格式的音频文件，以前导入的mp3音频听起来实在糊耳朵，这里提取为m4a音频。 如果有其他需要当然也可以提取为其他格式的音频，比如aac、wav等高音质格式。","link":"/2022/08/22/2022-8-22/"},{"title":"","text":"论文阅读：Texture-based Error Analysis for Image Super-Resolution Abstract 图像超分任务的通常采用PSNR或SSIM这样的单值评估指标，从这些指标中几乎无法得知有关产生错误的源头和模型的行为的信息。因此本文将优先考虑可解释性，从多方面聚焦错误分析。本文的主要贡献在于通过一个纹理分类器给patches分配语义标签，以识别全局或局部的SR error的来源。然后依此完成了以下abc工作。。。。 Introduction 近年来图像超分模型的训练评估标准做法一直没什么大的改变。先用DIV2K HR优化网络，然后在五个benchmark数据集上（Set5、Set14、B100、Urban100、Manga109）以两个指标（PSNR、SSIM）进行评估。 上述方法的limitations： 该范式用单一的数值代表模型在整个数据集上的表现，一张图片就存在巨大的可变性，更不用说整个数据集了，仅用PSNR不太合适。 由于不清楚数据集的具体描述，这样得出的结果可能存在误导，比如模型指标的高低可能与数据集中不同内容的图像块强相关。 由于对数据集没有探查过程，也无法得知其是否与要完成的任务相对应。 神经网络总是会做出过于自信的预测改变原图像中的语义信息，如图1左，重建网络改变了原图像上的数字“094”；如图1右，鸟身上线的数量、方向、宽度都被改变了。 本文细致观察SR框架中的每一步中产生错误的源头，宗旨是以人为本，主打一个简单易懂易于表达。本文的分析模式如图2，通过把自然图像流形分类到不同的语义集群中来分析数据分布、错误分布以及语义一致性。 本文的实验解决了以下问题： 每个SR数据集编码了怎样的语义信息？训练和评估所用的数据集匹配性如何？ 模型的错误发生在和哪些语义组有关的地方？ SR模型是否保留了patches的语义信息？ Method 本文的Method部分长得和其他文章的Experiments部分比较类似，主要讲述一些实现的细节。本文的主要部分还是对各种指标的对比和探讨，侧重于实验部分。 Texture Classifier Training Data：使用Describable Textures Dataset（DTD）训练纹理分类器，下载地址：https://www.robots.ox.ac.uk/~vgg/data/dtd/ 。该数据集由47种纹理的5640张图像组成，图像大小在300×300到640×640不等。 Architecture：纹理分类器需要保持空间不变性，本文选择deep texture encoding network（DeepTEN）作为纹理分类器的基础架构，backbone是ResNet50. Texture Embedding 对于数据集上的每张图像随机选取50个128×128的patches，DIV2K有800张图像，这样就有40000个patches。聚类使用标准K-Means算法聚成15类，Uniform Manifold Approximation and Projection (UMAP)用来可视化聚类结果。 Experiments Data and Texture Understanding Method 1: PSNR vs. Entropy 图像的熵指图像中每个像素所包含的信息量，通常用来描述一张图像的复杂性，并认为其与PSNR成反比。然而如图3所示，虽然整体趋势如此，但是PSNR值对应熵的跨度区间非常大，存在很大的冗余，单一地用熵或是PSNR作为描述图像复杂性和复原难度的指标是不合理的。 Method 2: PSNR vs. Entropy by Texture Labels 下面对不同纹理标签patches中的熵和PSNR进行分析。图4中列出了bicubic PSNR最高和最低的三种纹理标签的示意图。可以看到最低的三种纹理样本几乎都聚集在右下象限内，即高熵低PSNR；最高的三种纹理分布在整个域内，其也存在冗余现象。下面会通过聚类对这些patches的纹理嵌入进行分组。 Method 3: Textrue Feature Manifold 图5中对DIV2K数据集的patches进行聚类并可视化，上面的图是分类器分类出的图像纹理标签的可视化，下面的图是对这些patches做聚类之后的可视化。图6将每个聚类中出现频率最高的label的patches展示了出来。这两个图中体现了两点：其一是聚类和原生label之间存在混叠现象，同一个聚类中存在多个labels的混叠，同一个label中亦是如此，这表明一定有潜藏的公共特征使得一个patch同时属于多个聚类和labels；其二是在评估SR模型时应当考虑多个指标。 Discussion 作者发现，SR模型最难以恢复的纹理的patches都有高熵低PSNR的特点，这些patches的熵与PSNR的比值大概都高于0.4。由此作者将比值高于0.4的patches都取出来，它们共占所有patches的20%，并用这些高比值patches去训练SR模型，如图7所示，这些仅用20%的数据训练的模型的效果竟优于使用所有数据训练的模型。这意味着，在构造训练数据时patches的选择不应当是完全随机的，也需要精心的挑选。上述现象也与网络倾向于学习可泛化的特征有关，当数据中存在80%的低复杂度patches时，网络会对学习更难的patches产生惰性。 SR Error Analysis Data Distribution by Texture Labels 图8展示出每个数据集中的每种label的百分比，这样的统计信息可以在一定程度上表现出各个数据集的语义特性，也可以对训练和评估用数据集进行比较，来判定是否存在域的偏移。比如Manga109数据集中占比最高的三种labels没有一个存在于DIV2K数据集排名前10的labels中，用Manga109来评测DIV2K训练的模型就会产生一定程度上的误导。除此之外，这种方法也可以检查数据集的多样性。 Error Distribution by Texture Labels 潜在陷阱：模型表现最好的语义群可能无法代表数据集。 图9中展示了每个benchmark数据集中平均PSNR最高和最低的5个labels，图中的白色数字表示超分模型与传统插值之间的PSNR的差距绝对值。 图中表现出了几个问题： 一些类的labels在每个数据集上都容易或难以复原，具有难度上的一致性，比如fibrous，每个数据集最末5个labels都有它。正因有这种一致性，patches的纹理分类才能一定程度上表示图像的复杂性。假如我们想制作一个具有挑战性的数据集，这时就可以选择表现最差的纹理类别占比高的图像。 图8中列出了每个数据集中出现频率最高的labels，这样的频率分布给出了数据集的一种描述，但是超分模型表现最好的5个labels和数据集中频率最高的labels基本是对不上的，比如crosshatched类在Urban100中频率第三高，但总是表现最差。作者将其归因于它们在训练集中不常见（又怪域偏移？）。 潜在陷阱：深度学习算法与传统插值之间的差距可能惊人的小。 作者从图9中的白色数字即超分模型与bicubic之间PSNR的差距中惊奇地发现，在很多labels中，DL的方法只比插值方法提升很小很小，比如Urban100中最简单的patches，EDSR仅提升了0.41dB。既然如此，我们在超分时就可以挑选出这些类的patches并用插值法计算，借此节约算力。 Semantic Consistency by Texture Labels 作者对超分前后的labels进行统计，如表1所示，大约只有30%~50%的patches能够保持自身的语义信息不变，而且bicubic方法的语义保持度惊人的低，可见它对语义存在巨大的破坏。作者举了一些保持了语义和没有保持的patches的例子，如图10。 Limitations 两点limitations：一是分析结果依赖于超参的选择，比如patch size，选择大尺寸能包含更多的partterns，但是精度会显著下降；二是纹理分类器测试精度只有72%，还需要进一步提升。","link":"/2022/08/26/2022-8-26/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","link":"/2022/08/21/hello-world/"}],"tags":[],"categories":[],"pages":[]}